<div align="center">
<h1>
WorldWander: Bridging Egocentric and Exocentric Worlds in Video Generation
</h1>


<div>
    <a href='' target='_blank' style='text-decoration:none'>Quanjian Song<sup>1,2*</sup></a>, &ensp;
    <a href='' target='_blank' style='text-decoration:none'>Yiren Song<sup>1,2*</sup></a>, &ensp;
    <a href='' target='_blank' style='text-decoration:none'>Kelly Peng<sup>2</sup></a>, &ensp;
    <a href='' target='_blank' style='text-decoration:none'>Yuan Gao<sup>2</sup></a>, &ensp;
    <a href='' target='_blank' style='text-decoration:none'>Mike Zheng Shou<sup>1,â€ </sup></a>
</div>

<div>
    <sup>1</sup>Show Lab, National University of Singapore,  &ensp;
    <sup>2</sup>First Intelligence,
    <br>
    <sub>
        <sup>*</sup>Equal contribution.   &ensp;
        <sup>â€ </sup>Corresponding authors.
    </sub>
</div>

<p align="center">
    <span>
        <a href="" target="_blank"> 
        <img src='' alt='Paper PDF'></a> &emsp;  &emsp; 
    </span>
    <span> 
        <a href='https://lulupig12138.github.io/WorldWander' target="_blank">
        <img src='' alt='Project Page'></a>  &emsp;  &emsp;
    </span>
    <span> 
        <a href='' target="_blank"> 
        <img src='' alt='Hugging Face'></a> &emsp;  &emsp;
    </span>
</p>

</div>



## ðŸŽ¬ Teaser
<b>TL;DR:</b> We propose WorldWander, an in-context learning framework tailored for translating between egocentric and exocentric worlds in video generation. We also release [EgoExo-8K](XXX), a large-scale dataset containing synchronized egocentricâ€“exocentric triplets. The teaser is shown below:
![Overall Framework](assets/teaser.png)



## ðŸ“– Overview
Video diffusion models have recently achieved remarkable progress in realism and controllability. However, achieving seamless video translation across different perspectives, such as first-person (egocentric) and third-person (exocentric), remains underexplored. Bridging these perspectives is crucial for filmmaking, embodied AI, and world models.
Motivated by this, we present <b>WorldWander</b>, an in-context learning framework tailored for translating between egocentric and exocentric worlds in video generation. Building upon advanced video diffusion transformers, WorldWander integrates (i) <i>In-Context Perspective Alignment</i> and (ii) <i>Collaborative Position Encoding</i> to efficiently model cross-view synchronization.
Overall framework is shown below:
![Overall Framework](assets/overall_pipeline.png)



## ðŸ¤— Datasets
To further support our task, we curate <b>[EgoExo-8K](XXX)</b>, a large-scale dataset containing synchronized egocentricâ€“exocentric triplets from both <i>synthetic</i> and <i>real-world</i> scenarios.
We show some examples below:
![Datasets Example](assets/datasets_example.png)



## ðŸ”§ Environment
```
git clone https://github.com/showlab/WorldWander.git
# Installation with the requirement.txt
conda create -n WorldWander python=3.10
conda activate WorldWander
pip install -r requirements.txt
# Installation with environment.yml
conda env create -f environment.yml
conda activate WorldWander
```



## ðŸš€ Try Inference
WorldWander is trained on the [wan2.2-TI2V-5B](https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B-Diffusers) model using 4 H200 GPUs, with a batch size of 4 per GPU.
To make it easier for you to use directly, we provide the following checkpoints for different tasks:

| Models                             | Links               | config                             |
| ---------------------------------- | ------------------- | ---------------------------------- |
| wan2.2-TI2V-5B_three2one_synthetic | ðŸ¤— [Huggingface](xxx) | configs/wan2-2_lora_three2one_synthetic.yaml |
| wan2.2-TI2V-5B_one2three_synthetic | ðŸ¤— [Huggingface](xxx) | configs/wan2-2_lora_one2three_synthetic.yaml |
| wan2.2-TI2V-5B_three2one_realworld | ðŸ¤— [Huggingface](xxx) | configs/wan2-2_lora_three2one_realworld.yaml |
| wan2.2-TI2V-5B_one2three_realworld | ðŸ¤— [Huggingface](xxx) | configs/wan2-2_lora_one2three_realworld.yaml |

You can download the specific checkpoint above and specify the corresponding config file for inference.
For convenience, we have provided the following example script:
```
bash scripts/inference_wan2.sh
```
Note that the parameter `ckpt_path` needs to be updated to the path of the checkpoint you downloaded.
<b>It is recommended to run this on a GPU with 80GB of VRAM to avoid out of memory.</b>



## ðŸ”¥ Custom Training
You can also train on your custom dataset. To achieve this, you first need to adjust the `first_video_root`, `third_video_root`, `ref_image_root`, and other parameters in corresponding `config` file. If necessary, you may need to modify the `CustomTrainDataset` class in `dataset/custom_dataset.py` according to the attributes of your own dataset.
For convenience, we have also provided the following training script:
```
bash scripts/train_wan2.sh
```



## ðŸŽ“ Bibtex
ðŸ‘‹ If you find this code helpful for your research, please cite:
```
XXX
```
